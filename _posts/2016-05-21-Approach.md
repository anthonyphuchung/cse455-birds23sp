---
layout: post
subtitle: Approach, Previous Work
---

Our overall objective and goal was to train a neural network classifier that was accurate at classifying bird species based on bird images in a reasonably short amount of time. In this sense, we decided not to aim for the greatest accuracy on the Kaggle leaderboard but rather the greatest accuracy within our training time.

As a general approach, we decided to modify existing neural network architectures in order to complete the more specific classification task of classifying bird species. We started by using ResNet18 and removed the last fully connected layer of the network. Then we re-trained the network with a stochastic gradient descent (SGD) optimizer on the kaggle bird data. After training this network several times with ten epochs using varying learning rate and decay, we found that the runtime to train these networks was far from reasonable.

From there, we decided to halve the number of epochs that the network trained for to five. We understood that this would have a negative impact on our accuracy and risk underfitting, but we wanted to explore the possibilities of a neural network architecture that took shorter amounts of time to train.

From there, we decided to test out using several different optimizers and network architectures with our data. For architecture, we decided to try our original ResNet18, along with AlexNet. And for optimizers, along with SGD, we also decided to try Adam optimizer, which tends to converge faster. We decided to test all these architectures, and find the one that yields the best results with five epochs of training. Then with that network, we test different hyperparameter values to find a best accuracy.

